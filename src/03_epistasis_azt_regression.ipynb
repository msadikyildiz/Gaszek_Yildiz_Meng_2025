{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import math\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = Path(\"/project/greencenter/Toprak_lab/shared/TEM1_Combinatorial_Mutagenesis/src/Epistasis\")\n",
    "\n",
    "#sign_pos = [19,  37,  67, 102, 162, 180, 235, 236, 237, 241, 261, 271, 272]\n",
    "sign_pos   = [21,  39,  69, 104, 164, 182, 237, 238, 240, 244, 265, 275, 276]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PROJECT_PATH / 'data/processed' / \"Epistasis_Combined.parquet\")\n",
    "# df = df[(df.Drug == \"AZT\") & (df.Concentration == 12.0)].reset_index(drop=True)\n",
    "df = df[(df.Drug == \"AZT\") & (df.Concentration == 36.0)].reset_index(drop=True)\n",
    "# df = df[(df.Drug == \"AZT\") & (df.Concentration == 108.0)].reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the genotype into individual amino acids\n",
    "df['genotype_split'] = df['Genotype'].apply(lambda x: list(x))\n",
    "\n",
    "# Create a dataframe with each position as a separate column\n",
    "genotype_df = pd.DataFrame(df['genotype_split'].tolist(), index=df.index)\n",
    "genotype_df.columns = [f'{sign_pos[i]}' for i in genotype_df.columns]\n",
    "\n",
    "# Concatenate the original dataframe with the new genotype dataframe\n",
    "df = pd.concat([df, genotype_df], axis=1)\n",
    "df_genotypes = df['Genotype']\n",
    "df = df.drop(['Genotype', 'genotype_split'], axis=1)\n",
    "\n",
    "# One-hot encode the positional columns\n",
    "df_encoded = pd.get_dummies(df, columns=genotype_df.columns)\n",
    "\n",
    "# # Drop all columns except the one-hot encoded columns\n",
    "# df_encoded = df_encoded[['21_L',\n",
    "#        '21_P', '39_K', '39_Q', '69_L', '69_M', '69_V', '104_E', '104_K',\n",
    "#        '164_H', '164_N', '164_R', '164_S', '182_M', '182_T', '237_A', '237_T',\n",
    "#        '238_G', '238_S', '240_E', '240_K', '244_C', '244_R', '244_S', '265_M',\n",
    "#        '265_T', '275_L', '275_Q', '275_R', '276_D', '276_N', 'Fitness']]\n",
    "\n",
    "# Drop all columns except the one-hot encoded columns and wildtype\n",
    "df_encoded = df_encoded[[\n",
    "       '21_P', '39_K', '69_L', '69_V', '104_K',\n",
    "       '164_H', '164_N', '164_S', '182_T', '237_T',\n",
    "       '238_S', '240_K', '244_C', '244_S', '265_M',\n",
    "       '275_L', '275_Q', '276_D', 'Fitness']]\n",
    "\n",
    "# Rename columns to follow the convention: WildtypePositionMutant\n",
    "# Create a mapping dictionary for the column renaming\n",
    "column_mapping = {\n",
    "    '21_P': 'L21P',\n",
    "    '39_K': 'Q39K',\n",
    "    '69_L': 'M69L',\n",
    "    '69_V': 'M69V',\n",
    "    '104_K': 'E104K',\n",
    "    '164_H': 'R164H',\n",
    "    '164_N': 'R164N',\n",
    "    '164_S': 'R164S',\n",
    "    '182_T': 'M182T',\n",
    "    '237_T': 'A237T',\n",
    "    '238_S': 'G238S',\n",
    "    '240_K': 'E240K',\n",
    "    '244_C': 'R244C',\n",
    "    '244_S': 'R244S',\n",
    "    '265_M': 'T265M',\n",
    "    '275_L': 'R275L',\n",
    "    '275_Q': 'R275Q',\n",
    "    '276_D': 'N276D',\n",
    "}\n",
    "\n",
    "# Rename the columns\n",
    "df_encoded = df_encoded.rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training sizes\n",
    "train_sizes = [0.000625, 0.00125, 0.0025, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.3, 0.7, 0.9]\n",
    "RMSD_scores, RMSD_errors = [], []\n",
    "R2_scores, R2_errors = [], []\n",
    "MAPE_scores, MAPE_errors = [], []\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "results_df = df[['Epistatic Term', 'Epistatic Order', 'Fitness']].copy()\n",
    "\n",
    "# Set number of repetitions for error calculation\n",
    "n_folds = 10  # Similar to PyCaret's default cross-validation\n",
    "\n",
    "for train_size in train_sizes:\n",
    "    # Track metrics across repeats\n",
    "    r2_values = []\n",
    "    rmsd_values = []\n",
    "    mape_values = []\n",
    "    \n",
    "    # Store predictions from last run for results_df\n",
    "    final_predictions = None\n",
    "    \n",
    "    # Run multiple times with different random seeds for error estimation\n",
    "    for i in range(n_folds):\n",
    "        # Split data into training and testing sets\n",
    "        X = df_encoded.drop('Fitness', axis=1)\n",
    "        y = df_encoded['Fitness']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, train_size=train_size, random_state=np.random.randint(1000, 100000))\n",
    "        \n",
    "        # Create and train LightGBM model\n",
    "        # Default parameters similar to PyCaret's defaults\n",
    "        model = lgbm.LGBMRegressor(\n",
    "            objective='regression',\n",
    "            n_estimators=100,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42+i,\n",
    "            verbose=-1\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmsd = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        # Handle potential division by zero in MAPE\n",
    "        mape = 100 * mean_absolute_percentage_error(y_test, y_pred)\n",
    "        \n",
    "        # Store metrics\n",
    "        r2_values.append(r2)\n",
    "        rmsd_values.append(rmsd)\n",
    "        mape_values.append(mape)\n",
    "        \n",
    "        # Save the last run's predictions\n",
    "        if i == n_folds - 1:\n",
    "            final_predictions = model.predict(X)\n",
    "    \n",
    "    # Calculate mean and standard deviation\n",
    "    r2_mean = np.mean(r2_values)\n",
    "    r2_std = np.std(r2_values)\n",
    "    rmsd_mean = np.mean(rmsd_values)\n",
    "    rmsd_std = np.std(rmsd_values)\n",
    "    mape_mean = np.mean(mape_values)\n",
    "    mape_std = np.std(mape_values)\n",
    "    \n",
    "    # Store metrics\n",
    "    R2_scores.append(r2_mean)\n",
    "    R2_errors.append(r2_std)\n",
    "    RMSD_scores.append(rmsd_mean)\n",
    "    RMSD_errors.append(rmsd_std)\n",
    "    MAPE_scores.append(mape_mean)\n",
    "    MAPE_errors.append(mape_std)\n",
    "    \n",
    "    # Store predictions in results dataframe\n",
    "    results_df[f'Predicted_Fitness_{train_size*100:.2f}%'] = final_predictions\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Training size: {len(X_train)} samples ({train_size*100:.2f}%)\")\n",
    "    print(f\"R2 score: {r2_mean:.3f} ± {r2_std:.3f}\")\n",
    "    print(f\"MAPE: {mape_mean:.3f} ± {mape_std:.3f}\")\n",
    "    print(f\"RMSD: {rmsd_mean:.3f} ± {rmsd_std:.3f}\\n\")\n",
    "\n",
    "# Create a DataFrame with the statistics\n",
    "stats_df = pd.DataFrame({\n",
    "    'Training_Size_Fraction': [f\"{size*100}%\" for size in train_sizes],\n",
    "    'Training_Size': [f\"{round(size*df_encoded.shape[0])}\" for size in train_sizes],\n",
    "    'R2_Score': R2_scores,\n",
    "    'R2_Error': R2_errors,\n",
    "    'RMSD': RMSD_scores,\n",
    "    'RMSD_Error': RMSD_errors,\n",
    "    'MAPE_scores': MAPE_scores,\n",
    "    'MAPE_errors': MAPE_errors\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "stats_df.to_csv(PROJECT_PATH / 'figures/azt_regression' / 'AZT_regression_learning_curve_statistics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure with Nature-style dimensions and resolution for two panels side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 2.8), dpi=150)\n",
    "\n",
    "# Use a professional color palette\n",
    "color_rmsd = 'hotpink'# '#0072B2'  # Blue color for RMSD\n",
    "color_r2 = 'hotpink' #D55E00'    # Orange/red color for R2\n",
    "\n",
    "# Plot RMSD scores with error bars (left panel)\n",
    "ax1.errorbar(stats_df[\"Training_Size_Fraction\"].str.rstrip('%').astype(float), \n",
    "             stats_df[\"RMSD\"], \n",
    "             yerr=stats_df[\"RMSD_Error\"],\n",
    "             fmt='o-',\n",
    "             color=color_rmsd,\n",
    "             capsize=2,\n",
    "             markersize=4,\n",
    "             linewidth=1.5,\n",
    "             elinewidth=0.8)\n",
    "\n",
    "# Set log scale for x-axis with proper limits\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlim(0.005, 100)\n",
    "ax1.set_ylim(0.2, 0.45)  # RMSD typically ranges from 0 to 1\n",
    "\n",
    "# Customize axes labels with Nature-style formatting\n",
    "ax1.set_xlabel('Training set size (%)', fontsize=9)\n",
    "ax1.set_ylabel('RMSD', fontsize=9)\n",
    "\n",
    "# Customize tick parameters\n",
    "ax1.tick_params(axis='both', which='major', labelsize=8, direction='out', length=3, width=0.5)\n",
    "ax1.set_yticks(np.arange(0.2, 0.45, 0.1))\n",
    "\n",
    "# Set x-axis tick labels to show actual values instead of scientific notation\n",
    "ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:g}'))\n",
    "\n",
    "# Add custom grid (lighter, more subtle)\n",
    "ax1.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add a box around the plot\n",
    "ax1.spines['top'].set_visible(True)\n",
    "ax1.spines['right'].set_visible(True)\n",
    "ax1.spines['left'].set_linewidth(0.5)\n",
    "ax1.spines['right'].set_linewidth(0.5)\n",
    "ax1.spines['top'].set_linewidth(0.5)\n",
    "ax1.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "# Plot R2 scores with error bars (right panel)\n",
    "ax2.errorbar(stats_df[\"Training_Size_Fraction\"].str.rstrip('%').astype(float), \n",
    "             stats_df[\"R2_Score\"], \n",
    "             yerr=stats_df[\"R2_Error\"],\n",
    "             fmt='o-',\n",
    "             color=color_r2,\n",
    "             capsize=2,\n",
    "             markersize=4,\n",
    "             linewidth=1.5,\n",
    "             elinewidth=0.8)\n",
    "\n",
    "# Set log scale for x-axis with proper limits\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlim(0.005, 100)\n",
    "\n",
    "# Customize axes labels with Nature-style formatting\n",
    "ax2.set_xlabel('Training set size (%)', fontsize=9)\n",
    "ax2.set_ylabel('R²', fontsize=9)\n",
    "\n",
    "# Customize tick parameters\n",
    "ax2.tick_params(axis='both', which='major', labelsize=8, direction='out', length=3, width=0.5)\n",
    "ax2.set_ylim(-0.1, 1.0)  # R2 typically ranges from 0 to 1 (can be negative for poor fits)\n",
    "\n",
    "# Set x-axis tick labels to show actual values instead of scientific notation\n",
    "ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x:g}'))\n",
    "\n",
    "# Add custom grid (lighter, more subtle)\n",
    "ax2.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add a box around the plot\n",
    "ax2.spines['top'].set_visible(True)\n",
    "ax2.spines['right'].set_visible(True)\n",
    "ax2.spines['left'].set_linewidth(0.5)\n",
    "ax2.spines['right'].set_linewidth(0.5)\n",
    "ax2.spines['top'].set_linewidth(0.5)\n",
    "ax2.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "# Tight layout to optimize space\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save high-resolution figure (uncomment if needed)\n",
    "plt.savefig(PROJECT_PATH  / 'figures/azt_regression/learning_curve_metrics.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Split data into training and testing sets\n",
    "X = df_encoded.drop('Fitness', axis=1)\n",
    "y = df_encoded['Fitness']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.1, random_state=np.random.randint(1000, 100000))\n",
    "\n",
    "# Create and train LightGBM model\n",
    "# Default parameters similar to PyCaret's defaults\n",
    "model = lgbm.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D histogram with specified bin size and axis limits\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "\n",
    "max_fitness = 7\n",
    "# Define the number of bins and the range\n",
    "num_bins = 44\n",
    "bin_range = (0, max_fitness)\n",
    "\n",
    "# Create bins using np.linspace\n",
    "bins = np.linspace(bin_range[0], bin_range[1], num_bins)\n",
    "\n",
    "# Calculate RMSD and R2\n",
    "rmsd = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Create a 2D histogram with the specified bins\n",
    "plt.plot([0, max_fitness+.1], [0, max_fitness+.1], 'k--', lw=1, alpha=0.95, label='y = x')\n",
    "hist, xedges, yedges, img = plt.hist2d(y_test, y_pred, bins=[bins, bins], cmap='RdPu', norm=LogNorm())\n",
    "plt.colorbar(img, label='Number of Mutants (log10 scale)')\n",
    "plt.grid(False)\n",
    "\n",
    "# Add RMSD and R2 text labels\n",
    "plt.text(0.05, 0.8, f'RMSD = {rmsd:.3f}', transform=plt.gca().transAxes, \n",
    "         fontsize=10, fontweight='bold', bbox=dict(facecolor='white', alpha=0.7))\n",
    "plt.text(0.05, 0.7, f'R² = {r2:.3f}', transform=plt.gca().transAxes, \n",
    "         fontsize=10, fontweight='bold', bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "plt.xlim(0, max_fitness)\n",
    "plt.ylim(0, max_fitness)\n",
    "plt.xticks(range(0, max_fitness+1, 1))\n",
    "plt.yticks(range(0, max_fitness+1, 1))\n",
    "plt.xlabel('Measured Fitness (au)',fontweight='bold', fontsize=12)\n",
    "plt.ylabel('Predicted Fitness (au)',fontweight='bold', fontsize=12)\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "\n",
    "# Save figure with high resolution\n",
    "plt.savefig(PROJECT_PATH / 'figures/azt_regression/fitness_prediction_2d_histogram.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals for train and test sets\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_residuals = y_train - y_train_pred\n",
    "test_residuals = y_test - y_pred\n",
    "\n",
    "# Create a publication-quality residual plot\n",
    "plt.figure(figsize=(8, 5.5))\n",
    "fig, ax = plt.subplots(figsize=(8, 5.5))\n",
    "\n",
    "# Plot train and test residuals with different colors\n",
    "ax.scatter(y_train_pred, train_residuals, s=30, alpha=0.5, color='#4878D0', \n",
    "           edgecolor='none', label='Training set')\n",
    "ax.scatter(y_pred, test_residuals, s=30, alpha=0.5, color='#EE854A', \n",
    "           edgecolor='none', label='Test set')\n",
    "\n",
    "# Add a horizontal line at y=0\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.8, alpha=0.7)\n",
    "\n",
    "# Calculate RMSE for both sets\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Add RMSE text annotations\n",
    "ax.text(0.05, 0.95, f'Train RMSE: {train_rmse:.4f}', transform=ax.transAxes,\n",
    "        fontsize=10, fontweight='bold', bbox=dict(facecolor='white', alpha=0.7))\n",
    "ax.text(0.05, 0.89, f'Test RMSE: {test_rmse:.4f}', transform=ax.transAxes,\n",
    "        fontsize=10, fontweight='bold', bbox=dict(facecolor='white', alpha=0.7))\n",
    "\n",
    "# Style the plot for Nature journal quality\n",
    "ax.set_xlabel('Predicted Fitness (au)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Residuals (Measured - Predicted)', fontsize=12, fontweight='bold')\n",
    "ax.tick_params(axis='both', which='major', labelsize=10, width=1)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(1)\n",
    "ax.spines['bottom'].set_linewidth(1)\n",
    "\n",
    "# Set axis limits\n",
    "# ax.set_xlim(0, 1.1)\n",
    "y_range = max(abs(train_residuals.min()), abs(train_residuals.max()), \n",
    "              abs(test_residuals.min()), abs(test_residuals.max()))\n",
    "# ax.set_ylim(-y_range*1.1, y_range*1.1)\n",
    "\n",
    "# Add legend with custom styling\n",
    "legend = ax.legend(frameon=True, fontsize=10, loc='lower right')\n",
    "legend.get_frame().set_linewidth(0.5)\n",
    "legend.get_frame().set_edgecolor('black')\n",
    "\n",
    "# Add grid for better readability\n",
    "ax.grid(True, linestyle='--', alpha=0.3)\n",
    "# ax.set_xlim([0,1.1])\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display summary statistics of residuals\n",
    "print(f\"Training set residuals - Mean: {train_residuals.mean():.4f}, Std: {train_residuals.std():.4f}\")\n",
    "print(f\"Test set residuals - Mean: {test_residuals.mean():.4f}, Std: {test_residuals.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming model is trained and you have X_test, y_test\n",
    "result = permutation_importance(model, X_test, y_test, n_repeats=30, random_state=42)\n",
    "\n",
    "# Sort features by importance\n",
    "sorted_idx = result.importances_mean.argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "# Get top N features\n",
    "N = 19\n",
    "top_N_idx = sorted_idx[:N]\n",
    "# Get the rest of the features and combine their importance\n",
    "rest_idx = sorted_idx[N:]\n",
    "rest_importances = result.importances[rest_idx]\n",
    "\n",
    "# Calculate mean importance for the rest of features\n",
    "rest_mean_importance = np.mean(rest_importances, axis=0)\n",
    "\n",
    "\n",
    "# Prepare data for plotting - ensure dimensions match\n",
    "top_features_data = result.importances[top_N_idx].T  # Shape: (n_repeats, 9)\n",
    "rest_features_data = rest_mean_importance.reshape(-1, 1)  # Shape: (n_repeats, 1)\n",
    "\n",
    "# Combine the data for plotting with matching dimensions\n",
    "if len(rest_idx) > 0:\n",
    "    plot_labels = list(X_test.columns[top_N_idx]) + [f\"Sum of {len(rest_idx)} features\"]\n",
    "    plot_data = np.concatenate([top_features_data, rest_features_data], axis=1)\n",
    "else:\n",
    "    plot_labels = list(X_test.columns[top_N_idx])\n",
    "    plot_data = top_features_data\n",
    "\n",
    "# Plot the boxplot\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.boxplot(plot_data, vert=False, labels=plot_labels)\n",
    "plt.xscale('log')\n",
    "plt.title(\"Permutation Importance\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(PROJECT_PATH / 'figures/azt_regression/permutation_importance.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "from typing import Literal\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.cluster\n",
    "import scipy.sparse\n",
    "import scipy.spatial\n",
    "from matplotlib.figure import Figure\n",
    "from packaging import version\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "from shap.utils import safe_isinstance\n",
    "from shap import Explanation\n",
    "from shap.plots import colors\n",
    "from shap.plots._labels import labels\n",
    "from shap.plots._utils import (\n",
    "    convert_color,\n",
    "    convert_ordering,\n",
    "    get_sort_order,\n",
    "    merge_nodes,\n",
    "    sort_inds,\n",
    ")\n",
    "from shap import Cohorts, Explanation\n",
    "from shap.utils import format_value, ordinal_str\n",
    "from shap.utils._exceptions import DimensionError\n",
    "from shap.plots._style import get_style\n",
    "from shap.plots._utils import convert_ordering, dendrogram_coords, get_sort_order, merge_nodes, sort_inds\n",
    "\n",
    "def beeswarm(\n",
    "    shap_values: Explanation,\n",
    "    max_display: int | None = 10,\n",
    "    order=Explanation.abs.mean(0),  # type: ignore\n",
    "    clustering=None,\n",
    "    cluster_threshold=0.5,\n",
    "    color=None,\n",
    "    axis_color=\"#333333\",\n",
    "    alpha: float = 1.0,\n",
    "    ax: pl.Axes | None = None,\n",
    "    show: bool = True,\n",
    "    log_scale: bool = False,\n",
    "    color_bar: bool = True,\n",
    "    s: float = 16,\n",
    "    plot_size: Literal[\"auto\"] | float | tuple[float, float] | None = \"auto\",\n",
    "    color_bar_label: str = labels[\"FEATURE_VALUE\"],\n",
    "    group_remaining_features: bool = True,\n",
    "):\n",
    "    \"\"\"Create a SHAP beeswarm plot, colored by feature values when they are provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shap_values : Explanation\n",
    "        This is an :class:`.Explanation` object containing a matrix of SHAP values\n",
    "        (# samples x # features).\n",
    "\n",
    "    max_display : int\n",
    "        How many top features to include in the plot (default is 10, or 7 for\n",
    "        interaction plots).\n",
    "\n",
    "    ax: matplotlib Axes\n",
    "        Axes object to draw the plot onto, otherwise uses the current Axes.\n",
    "\n",
    "    show : bool\n",
    "        Whether :external+mpl:func:`matplotlib.pyplot.show()` is called before returning.\n",
    "        Setting this to ``False`` allows the plot to be customized further\n",
    "        after it has been created, returning the current axis via\n",
    "        :external+mpl:func:`matplotlib.pyplot.gca()`.\n",
    "\n",
    "    color_bar : bool\n",
    "        Whether to draw the color bar (legend).\n",
    "\n",
    "    s : float\n",
    "        What size to make the markers. For further information, see ``s`` in\n",
    "        :external+mpl:func:`matplotlib.pyplot.scatter`.\n",
    "\n",
    "    plot_size : \"auto\" (default), float, (float, float), or None\n",
    "        What size to make the plot. By default, the size is auto-scaled based on the\n",
    "        number of features that are being displayed. Passing a single float will cause\n",
    "        each row to be that many inches high. Passing a pair of floats will scale the\n",
    "        plot by that number of inches. If ``None`` is passed, then the size of the\n",
    "        current figure will be left unchanged. If ``ax`` is not ``None``, then passing\n",
    "        ``plot_size`` will raise a :exc:`ValueError`.\n",
    "\n",
    "    group_remaining_features: bool\n",
    "        If there are more features than ``max_display``, then plot a row representing\n",
    "        the sum of SHAP values of all remaining features. Default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ax: matplotlib Axes\n",
    "        Returns the :external+mpl:class:`~matplotlib.axes.Axes` object with the plot drawn onto it. Only\n",
    "        returned if ``show=False``.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    See `beeswarm plot examples <https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html>`_.\n",
    "\n",
    "    \"\"\"\n",
    "    if not isinstance(shap_values, Explanation):\n",
    "        emsg = \"The beeswarm plot requires an `Explanation` object as the `shap_values` argument.\"\n",
    "        raise TypeError(emsg)\n",
    "\n",
    "    sv_shape = shap_values.shape\n",
    "    if len(sv_shape) == 1:\n",
    "        emsg = (\n",
    "            \"The beeswarm plot does not support plotting a single instance, please pass \"\n",
    "            \"an explanation matrix with many instances!\"\n",
    "        )\n",
    "        raise ValueError(emsg)\n",
    "    elif len(sv_shape) > 2:\n",
    "        emsg = (\n",
    "            \"The beeswarm plot does not support plotting explanations with instances that have more than one dimension!\"\n",
    "        )\n",
    "        raise ValueError(emsg)\n",
    "\n",
    "    if ax and plot_size:\n",
    "        emsg = (\n",
    "            \"The beeswarm plot does not support passing an axis and adjusting the plot size. \"\n",
    "            \"To adjust the size of the plot, set plot_size to None and adjust the size on the original figure the axes was part of\"\n",
    "        )\n",
    "        raise ValueError(emsg)\n",
    "\n",
    "    shap_exp = shap_values\n",
    "    # we make a copy here, because later there are places that might modify this array\n",
    "    values = np.copy(shap_exp.values)\n",
    "    features = shap_exp.data\n",
    "    if scipy.sparse.issparse(features):\n",
    "        features = features.toarray()\n",
    "    feature_names = shap_exp.feature_names\n",
    "    # if out_names is None: # TODO: waiting for slicer support\n",
    "    #     out_names = shap_exp.output_names\n",
    "\n",
    "    order = convert_ordering(order, values)\n",
    "\n",
    "    # default color:\n",
    "    if color is None:\n",
    "        if features is not None:\n",
    "            color = colors.red_blue\n",
    "        else:\n",
    "            color = colors.blue_rgb\n",
    "    color = convert_color(color)\n",
    "\n",
    "    idx2cat = None\n",
    "    # convert from a DataFrame or other types\n",
    "    if isinstance(features, pd.DataFrame):\n",
    "        if feature_names is None:\n",
    "            feature_names = features.columns\n",
    "        # feature index to category flag\n",
    "        idx2cat = features.dtypes.astype(str).isin([\"object\", \"category\"]).tolist()\n",
    "        features = features.values\n",
    "    elif isinstance(features, list):\n",
    "        if feature_names is None:\n",
    "            feature_names = features\n",
    "        features = None\n",
    "    elif (features is not None) and len(features.shape) == 1 and feature_names is None:\n",
    "        feature_names = features\n",
    "        features = None\n",
    "\n",
    "    num_features = values.shape[1]\n",
    "\n",
    "    if features is not None:\n",
    "        shape_msg = \"The shape of the shap_values matrix does not match the shape of the provided data matrix.\"\n",
    "        if num_features - 1 == features.shape[1]:\n",
    "            shape_msg += (\n",
    "                \" Perhaps the extra column in the shap_values matrix is the \"\n",
    "                \"constant offset? If so, just pass shap_values[:,:-1].\"\n",
    "            )\n",
    "            raise ValueError(shape_msg)\n",
    "        if num_features != features.shape[1]:\n",
    "            raise ValueError(shape_msg)\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = np.array([labels[\"FEATURE\"] % str(i) for i in range(num_features)])\n",
    "\n",
    "    if ax is None:\n",
    "        ax = pl.gca()\n",
    "    fig = ax.get_figure()\n",
    "    assert isinstance(fig, Figure)  # type narrowing for mypy\n",
    "\n",
    "    if log_scale:\n",
    "        ax.set_xscale(\"symlog\")\n",
    "\n",
    "    if clustering is None:\n",
    "        partition_tree = getattr(shap_values, \"clustering\", None)\n",
    "        if partition_tree is not None and partition_tree.var(0).sum() == 0:\n",
    "            partition_tree = partition_tree[0]\n",
    "        else:\n",
    "            partition_tree = None\n",
    "    elif clustering is False:\n",
    "        partition_tree = None\n",
    "    else:\n",
    "        partition_tree = clustering\n",
    "\n",
    "    if partition_tree is not None:\n",
    "        if partition_tree.shape[1] != 4:\n",
    "            emsg = (\n",
    "                \"The clustering provided by the Explanation object does not seem to \"\n",
    "                \"be a partition tree (which is all shap.plots.bar supports)!\"\n",
    "            )\n",
    "            raise ValueError(emsg)\n",
    "\n",
    "    # determine how many top features we will plot\n",
    "    if max_display is None:\n",
    "        max_display = len(feature_names)\n",
    "    num_features = min(max_display, len(feature_names))\n",
    "\n",
    "    # iteratively merge nodes until we can cut off the smallest feature values to stay within\n",
    "    # num_features without breaking a cluster tree\n",
    "    orig_inds = [[i] for i in range(len(feature_names))]\n",
    "    orig_values = values.copy()\n",
    "    while True:\n",
    "        feature_order = convert_ordering(order, Explanation(np.abs(values)))\n",
    "        if partition_tree is not None:\n",
    "            # compute the leaf order if we were to show (and so have the ordering respect) the whole partition tree\n",
    "            clust_order = sort_inds(partition_tree, np.abs(values))\n",
    "\n",
    "            # now relax the requirement to match the partition tree ordering for connections above cluster_threshold\n",
    "            dist = scipy.spatial.distance.squareform(scipy.cluster.hierarchy.cophenet(partition_tree))\n",
    "            feature_order = get_sort_order(dist, clust_order, cluster_threshold, feature_order)\n",
    "\n",
    "            # if the last feature we can display is connected in a tree the next feature then we can't just cut\n",
    "            # off the feature ordering, so we need to merge some tree nodes and then try again.\n",
    "            if (\n",
    "                max_display < len(feature_order)\n",
    "                and dist[feature_order[max_display - 1], feature_order[max_display - 2]] <= cluster_threshold\n",
    "            ):\n",
    "                # values, partition_tree, orig_inds = merge_nodes(values, partition_tree, orig_inds)\n",
    "                partition_tree, ind1, ind2 = merge_nodes(np.abs(values), partition_tree)\n",
    "                for _ in range(len(values)):\n",
    "                    values[:, ind1] += values[:, ind2]\n",
    "                    values = np.delete(values, ind2, 1)\n",
    "                    orig_inds[ind1] += orig_inds[ind2]\n",
    "                    del orig_inds[ind2]\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # here we build our feature names, accounting for the fact that some features might be merged together\n",
    "    feature_inds = feature_order[:max_display]\n",
    "    feature_names_new = []\n",
    "    for inds in orig_inds:\n",
    "        if len(inds) == 1:\n",
    "            feature_names_new.append(feature_names[inds[0]])\n",
    "        elif len(inds) <= 2:\n",
    "            feature_names_new.append(\" + \".join([feature_names[i] for i in inds]))\n",
    "        else:\n",
    "            max_ind = np.argmax(np.abs(orig_values).mean(0)[inds])\n",
    "            feature_names_new.append(f\"{feature_names[inds[max_ind]]} + {len(inds) - 1} other features\")\n",
    "    feature_names = feature_names_new\n",
    "\n",
    "    # see how many individual (vs. grouped at the end) features we are plotting\n",
    "    include_grouped_remaining = num_features < len(values[0]) and group_remaining_features\n",
    "    if include_grouped_remaining:\n",
    "        num_cut = np.sum([len(orig_inds[feature_order[i]]) for i in range(num_features - 1, len(values[0]))])\n",
    "        values[:, feature_order[num_features - 1]] = np.sum(\n",
    "            [values[:, feature_order[i]] for i in range(num_features - 1, len(values[0]))], 0\n",
    "        )\n",
    "\n",
    "    # build our y-tick labels\n",
    "    yticklabels = [feature_names[i] for i in feature_inds]\n",
    "    if include_grouped_remaining:\n",
    "        yticklabels[-1] = f\"Sum of {num_cut} other features\"\n",
    "\n",
    "    row_height = 0.5\n",
    "    if plot_size == \"auto\":\n",
    "        fig.set_size_inches(8, min(len(feature_order), max_display) * row_height + 1.5)\n",
    "    elif isinstance(plot_size, (list, tuple)):\n",
    "        fig.set_size_inches(plot_size[0], plot_size[1])\n",
    "    elif plot_size is not None:\n",
    "        fig.set_size_inches(8, min(len(feature_order), max_display) * plot_size + 1.5)\n",
    "    ax.axvline(x=0, color=\"#999999\", zorder=-1)\n",
    "\n",
    "    # Track if any binary features were found\n",
    "    binary_features_found = False\n",
    "    legend_added = False\n",
    "\n",
    "    # make the beeswarm dots\n",
    "    for pos, i in enumerate(reversed(feature_inds)):\n",
    "        ax.axhline(y=pos, color=\"#cccccc\", lw=0.5, dashes=(1, 5), zorder=-1)\n",
    "        shaps = values[:, i]\n",
    "        fvalues = None if features is None else features[:, i]\n",
    "        f_inds = np.arange(len(shaps))\n",
    "        np.random.shuffle(f_inds)\n",
    "        if fvalues is not None:\n",
    "            fvalues = fvalues[f_inds]\n",
    "        shaps = shaps[f_inds]\n",
    "        colored_feature = True\n",
    "        try:\n",
    "            if idx2cat is not None and idx2cat[i]:  # check categorical feature\n",
    "                colored_feature = False\n",
    "            else:\n",
    "                fvalues = np.array(fvalues, dtype=np.float64)  # make sure this can be numeric\n",
    "        except Exception:\n",
    "            colored_feature = False\n",
    "        N = len(shaps)\n",
    "        # hspacing = (np.max(shaps) - np.min(shaps)) / 200\n",
    "        # curr_bin = []\n",
    "        nbins = 100\n",
    "        quant = np.round(nbins * (shaps - np.min(shaps)) / (np.max(shaps) - np.min(shaps) + 1e-8))\n",
    "        inds_ = np.argsort(quant + np.random.randn(N) * 1e-6)\n",
    "        layer = 0\n",
    "        last_bin = -1\n",
    "        ys = np.zeros(N)\n",
    "        for ind in inds_:\n",
    "            if quant[ind] != last_bin:\n",
    "                layer = 0\n",
    "            ys[ind] = np.ceil(layer / 2) * ((layer % 2) * 2 - 1)\n",
    "            layer += 1\n",
    "            last_bin = quant[ind]\n",
    "        ys *= 0.9 * (row_height / np.max(ys + 1))\n",
    "\n",
    "        # Check if we have binary features (0 or 1)\n",
    "        if fvalues is not None and colored_feature is True:\n",
    "            is_binary = np.all(np.isin(np.unique(fvalues), [0, 1]))\n",
    "            \n",
    "            if is_binary:\n",
    "                # For binary features, use a legend instead of colorbar\n",
    "                binary_features_found = True\n",
    "                wildtype_mask = (fvalues == 0)\n",
    "                mutated_mask = (fvalues == 1)\n",
    "                \n",
    "                # Use black for wildtype and a second color for mutated\n",
    "                mutated_color = 'red'#'#008080'  # A nice blue color\n",
    "                \n",
    "                s = 4\n",
    "                alpha = 0.5\n",
    "                # Plot wildtype\n",
    "                if np.any(wildtype_mask):\n",
    "                    ax.scatter(\n",
    "                        shaps[wildtype_mask],\n",
    "                        pos + ys[wildtype_mask],\n",
    "                        color='black',\n",
    "                        s=s,\n",
    "                        alpha=alpha,\n",
    "                        linewidth=0,\n",
    "                        zorder=3,\n",
    "                        rasterized=len(shaps) > 50000,\n",
    "                        label='Wildtype' if not legend_added else \"\"\n",
    "                    )\n",
    "                \n",
    "                # Plot mutated\n",
    "                if np.any(mutated_mask):\n",
    "                    ax.scatter(\n",
    "                        shaps[mutated_mask],\n",
    "                        pos + ys[mutated_mask],\n",
    "                        color=mutated_color,\n",
    "                        s=s,\n",
    "                        alpha=1,\n",
    "                        linewidth=0,\n",
    "                        zorder=5,\n",
    "                        rasterized=len(shaps) > 50000,\n",
    "                        label='Mutated' if not legend_added else \"\"\n",
    "                    )\n",
    "                \n",
    "                # Add the legend just once\n",
    "                if not legend_added and color_bar:\n",
    "                    legend_added = True\n",
    "            elif safe_isinstance(color, \"matplotlib.colors.Colormap\"):\n",
    "                # For non-binary features, use the original colormap approach\n",
    "                # trim the color range, but prevent the color range from collapsing\n",
    "                vmin = np.nanpercentile(fvalues, 5)\n",
    "                vmax = np.nanpercentile(fvalues, 95)\n",
    "                if vmin == vmax:\n",
    "                    vmin = np.nanpercentile(fvalues, 1)\n",
    "                    vmax = np.nanpercentile(fvalues, 99)\n",
    "                    if vmin == vmax:\n",
    "                        vmin = np.min(fvalues)\n",
    "                        vmax = np.max(fvalues)\n",
    "                if vmin > vmax:  # fixes rare numerical precision issues\n",
    "                    vmin = vmax\n",
    "\n",
    "                # plot the nan fvalues in the interaction feature as grey\n",
    "                nan_mask = np.isnan(fvalues)\n",
    "                ax.scatter(\n",
    "                    shaps[nan_mask],\n",
    "                    pos + ys[nan_mask],\n",
    "                    color=\"#777777\",\n",
    "                    s=s,\n",
    "                    alpha=alpha,\n",
    "                    linewidth=0,\n",
    "                    zorder=3,\n",
    "                    rasterized=len(shaps) > 500,\n",
    "                )\n",
    "\n",
    "                # plot the non-nan fvalues colored by the trimmed feature value\n",
    "                cvals = fvalues[np.invert(nan_mask)].astype(np.float64)\n",
    "                cvals_imp = cvals.copy()\n",
    "                cvals_imp[np.isnan(cvals)] = (vmin + vmax) / 2.0\n",
    "                cvals[cvals_imp > vmax] = vmax\n",
    "                cvals[cvals_imp < vmin] = vmin\n",
    "                ax.scatter(\n",
    "                    shaps[np.invert(nan_mask)],\n",
    "                    pos + ys[np.invert(nan_mask)],\n",
    "                    cmap=color,\n",
    "                    vmin=vmin,\n",
    "                    vmax=vmax,\n",
    "                    s=s,\n",
    "                    c=cvals,\n",
    "                    alpha=alpha,\n",
    "                    linewidth=0,\n",
    "                    zorder=3,\n",
    "                    rasterized=len(shaps) > 500,\n",
    "                )\n",
    "        else:\n",
    "            if safe_isinstance(color, \"matplotlib.colors.Colormap\") and hasattr(color, \"colors\"):\n",
    "                color = color.colors\n",
    "            ax.scatter(\n",
    "                shaps,\n",
    "                pos + ys,\n",
    "                s=s,\n",
    "                alpha=alpha,\n",
    "                linewidth=0,\n",
    "                zorder=3,\n",
    "                color=color if colored_feature else \"#777777\",\n",
    "                rasterized=len(shaps) > 500,\n",
    "            )\n",
    "        \n",
    "        if binary_features_found:\n",
    "            # Add legend only if it's not already there\n",
    "            if not ax.get_legend():\n",
    "                # Create dummy larger points for legend\n",
    "                legend_elements = [\n",
    "                    pl.Line2D([0], [0], marker='o', color='black', label='Wildtype',\n",
    "                        markerfacecolor='black', markersize=8, linestyle='None'),\n",
    "                    pl.Line2D([0], [0], marker='o', color=mutated_color, label='Mutated',\n",
    "                        markerfacecolor=mutated_color, markersize=8, linestyle='None')\n",
    "                ]\n",
    "                leg = ax.legend(handles=legend_elements, loc='lower right', frameon=True)\n",
    "                leg.get_frame().set_linewidth(0.5)\n",
    "        elif safe_isinstance(color, \"matplotlib.colors.Colormap\"):\n",
    "            # Use colorbar for continuous features \n",
    "            import matplotlib.cm as cm\n",
    "            m = cm.ScalarMappable(cmap=color)\n",
    "            m.set_array([0, 1])\n",
    "            cb = fig.colorbar(m, ax=ax, ticks=[0, 1], aspect=80)\n",
    "            cb.set_ticklabels([labels[\"FEATURE_VALUE_LOW\"], labels[\"FEATURE_VALUE_HIGH\"]])\n",
    "            cb.set_label(color_bar_label, size=12, labelpad=0)\n",
    "            cb.ax.tick_params(labelsize=11, length=0)\n",
    "            cb.set_alpha(1)\n",
    "            cb.outline.set_visible(False)\n",
    "\n",
    "    ax.xaxis.set_ticks_position(\"bottom\")\n",
    "    ax.yaxis.set_ticks_position(\"none\")\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.tick_params(color=axis_color, labelcolor=axis_color)\n",
    "    ax.set_yticks(range(len(feature_inds)), list(reversed(yticklabels)), fontsize=13)\n",
    "    ax.tick_params(\"y\", length=20, width=0.5, which=\"major\")\n",
    "    ax.tick_params(\"x\", labelsize=11)\n",
    "    ax.set_ylim(-1, len(feature_inds))\n",
    "    #ax.set_xlabel(labels[\"VALUE\"], fontsize=13)\n",
    "    ax.set_xlabel(\"SHAP value\", fontsize=13)\n",
    "    if show:\n",
    "        pl.show()\n",
    "    else:\n",
    "        return ax\n",
    "\n",
    "def bar(\n",
    "    shap_values,\n",
    "    max_display=10,\n",
    "    order=Explanation.abs,\n",
    "    clustering=None,\n",
    "    clustering_cutoff=0.5,\n",
    "    show_data=\"auto\",\n",
    "    ax=None,\n",
    "    show=True,\n",
    "):\n",
    "    \"\"\"Create a bar plot of a set of SHAP values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shap_values : shap.Explanation or shap.Cohorts or dictionary of shap.Explanation objects\n",
    "        Passing a multi-row :class:`.Explanation` object creates a global\n",
    "        feature importance plot.\n",
    "\n",
    "        Passing a single row of an explanation (i.e. ``shap_values[0]``) creates\n",
    "        a local feature importance plot.\n",
    "\n",
    "        Passing a dictionary of Explanation objects will create a multiple-bar\n",
    "        plot with one bar type for each of the cohorts represented by the\n",
    "        explanation objects.\n",
    "    max_display : int\n",
    "        How many top features to include in the bar plot (default is 10).\n",
    "    order : OpChain or numpy.ndarray\n",
    "        A function that returns a sort ordering given a matrix of SHAP values\n",
    "        and an axis, or a direct sample ordering given as a ``numpy.ndarray``.\n",
    "\n",
    "        By default, take the absolute value.\n",
    "    clustering: np.ndarray or None\n",
    "        A partition tree, as returned by :func:`shap.utils.hclust`\n",
    "    clustering_cutoff: float\n",
    "        Controls how much of the clustering structure is displayed.\n",
    "    show_data: bool or str\n",
    "        Controls if data values are shown as part of the y tick labels. If\n",
    "        \"auto\", we show the data only when there are no transforms.\n",
    "    ax: matplotlib Axes\n",
    "        Axes object to draw the plot onto, otherwise uses the current Axes.\n",
    "    show : bool\n",
    "        Whether :external+mpl:func:`matplotlib.pyplot.show()` is called before returning.\n",
    "        Setting this to ``False`` allows the plot\n",
    "        to be customized further after it has been created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ax: matplotlib Axes\n",
    "        Returns the :external+mpl:class:`~matplotlib.axes.Axes` object with the plot drawn onto it. Only\n",
    "        returned if ``show=False``.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    See `bar plot examples <https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/bar.html>`_.\n",
    "\n",
    "    \"\"\"\n",
    "    postive_color = \"gray\"\n",
    "    \n",
    "    style = get_style()\n",
    "    # convert Explanation objects to dictionaries\n",
    "    if isinstance(shap_values, Explanation):\n",
    "        cohorts = {\"\": shap_values}\n",
    "    elif isinstance(shap_values, Cohorts):\n",
    "        cohorts = shap_values.cohorts\n",
    "    elif isinstance(shap_values, dict):\n",
    "        cohorts = shap_values\n",
    "    else:\n",
    "        emsg = (\n",
    "            \"The shap_values argument must be an Explanation object, Cohorts \"\n",
    "            \"object, or dictionary of Explanation objects!\"\n",
    "        )\n",
    "        raise TypeError(emsg)\n",
    "\n",
    "    # unpack our list of Explanation objects we need to plot\n",
    "    cohort_labels = list(cohorts.keys())\n",
    "    cohort_exps = list(cohorts.values())\n",
    "    for i, exp in enumerate(cohort_exps):\n",
    "        if not isinstance(exp, Explanation):\n",
    "            emsg = (\n",
    "                \"The shap_values argument must be an Explanation object, Cohorts \"\n",
    "                \"object, or dictionary of Explanation objects!\"\n",
    "            )\n",
    "            raise TypeError(emsg)\n",
    "\n",
    "        if len(exp.shape) == 2:\n",
    "            # collapse the Explanation arrays to be of shape (#features,)\n",
    "            cohort_exps[i] = exp.abs.mean(0)\n",
    "        if cohort_exps[i].shape != cohort_exps[0].shape:\n",
    "            emsg = \"When passing several Explanation objects, they must all have the same number of feature columns!\"\n",
    "            raise DimensionError(emsg)\n",
    "        # TODO: check other attributes for equality? like feature names perhaps? probably clustering as well.\n",
    "\n",
    "    # unpack the Explanation object\n",
    "    features = cohort_exps[0].display_data if cohort_exps[0].display_data is not None else cohort_exps[0].data\n",
    "    feature_names = cohort_exps[0].feature_names\n",
    "    if clustering is None:\n",
    "        partition_tree = getattr(cohort_exps[0], \"clustering\", None)\n",
    "    elif clustering is False:\n",
    "        partition_tree = None\n",
    "    else:\n",
    "        partition_tree = clustering\n",
    "    if partition_tree is not None:\n",
    "        if len(partition_tree.shape) != 2 or partition_tree.shape[1] != 4:\n",
    "            raise TypeError(\n",
    "                \"The clustering provided by the Explanation object does not seem to be a \"\n",
    "                \"partition tree, which is all shap.plots.bar supports.\"\n",
    "            )\n",
    "    op_history: list[OpHistoryItem] = cohort_exps[0].op_history\n",
    "    values = np.array([cohort_exps[i].values for i in range(len(cohort_exps))])\n",
    "\n",
    "    if len(values[0]) == 0:\n",
    "        raise ValueError(\"The passed Explanation is empty, so there is nothing to plot.\")\n",
    "\n",
    "    # we show the data on auto only when there are no transforms (excluding getitem calls)\n",
    "    if show_data == \"auto\":\n",
    "        transforms = [op for op in op_history if op.name != \"__getitem__\"]\n",
    "        show_data = len(transforms) == 0\n",
    "\n",
    "    # TODO: Rather than just show the \"1st token\", \"2nd token\", etc. it would be better to show the \"Instance 0's 1st but\", etc\n",
    "    if issubclass(type(feature_names), str):\n",
    "        feature_names = [ordinal_str(i) + \" \" + feature_names for i in range(len(values[0]))]\n",
    "\n",
    "    # build our auto xlabel based on the transform history of the Explanation object\n",
    "    xlabel = \"SHAP value\"\n",
    "    for op in op_history:\n",
    "        if op.name == \"abs\":\n",
    "            xlabel = f\"|{xlabel}|\"\n",
    "        elif op.name == \"__getitem__\":\n",
    "            pass  # no need for slicing to effect our label, it will be used later to find the sizes of cohorts\n",
    "        else:\n",
    "            xlabel = f\"{op.name}({xlabel})\"\n",
    "\n",
    "    # find how many instances are in each cohort (if they were created from an Explanation object)\n",
    "    cohort_sizes = []\n",
    "    for exp in cohort_exps:\n",
    "        for op in exp.op_history:\n",
    "            if op.collapsed_instances:  # see if this if the first op to collapse the instances\n",
    "                cohort_sizes.append(op.prev_shape[0])\n",
    "                break\n",
    "\n",
    "    # unwrap any pandas series\n",
    "    if isinstance(features, pd.Series):\n",
    "        if feature_names is None:\n",
    "            feature_names = list(features.index)\n",
    "        features = features.values\n",
    "\n",
    "    # ensure we at least have default feature names\n",
    "    if feature_names is None:\n",
    "        feature_names = np.array([labels[\"FEATURE\"] % str(i) for i in range(len(values[0]))])\n",
    "\n",
    "    # determine how many top features we will plot\n",
    "    if max_display is None:\n",
    "        max_display = len(feature_names)\n",
    "    num_features = min(max_display, len(values[0]))\n",
    "    max_display = min(max_display, num_features)\n",
    "\n",
    "    # iteratively merge nodes until we can cut off the smallest feature values to stay within\n",
    "    # num_features without breaking a cluster tree\n",
    "    orig_inds = [[i] for i in range(len(values[0]))]\n",
    "    orig_values = values.copy()\n",
    "    while True:\n",
    "        feature_order = np.argsort(\n",
    "            np.mean([np.argsort(convert_ordering(order, Explanation(values[i]))) for i in range(values.shape[0])], 0)\n",
    "        )\n",
    "        if partition_tree is not None:\n",
    "            # compute the leaf order if we were to show (and so have the ordering respect) the whole partition tree\n",
    "            clust_order = sort_inds(partition_tree, np.abs(values).mean(0))\n",
    "\n",
    "            # now relax the requirement to match the partition tree ordering for connections above clustering_cutoff\n",
    "            dist = scipy.spatial.distance.squareform(scipy.cluster.hierarchy.cophenet(partition_tree))\n",
    "            feature_order = get_sort_order(dist, clust_order, clustering_cutoff, feature_order)\n",
    "\n",
    "            # if the last feature we can display is connected in a tree the next feature then we can't just cut\n",
    "            # off the feature ordering, so we need to merge some tree nodes and then try again.\n",
    "            if (\n",
    "                max_display < len(feature_order)\n",
    "                and dist[feature_order[max_display - 1], feature_order[max_display - 2]] <= clustering_cutoff\n",
    "            ):\n",
    "                # values, partition_tree, orig_inds = merge_nodes(values, partition_tree, orig_inds)\n",
    "                partition_tree, ind1, ind2 = merge_nodes(np.abs(values).mean(0), partition_tree)\n",
    "                for _ in range(len(values)):\n",
    "                    values[:, ind1] += values[:, ind2]\n",
    "                    values = np.delete(values, ind2, 1)\n",
    "                    orig_inds[ind1] += orig_inds[ind2]\n",
    "                    del orig_inds[ind2]\n",
    "            else:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # here we build our feature names, accounting for the fact that some features might be merged together\n",
    "    feature_inds = feature_order[:max_display]\n",
    "    y_pos = np.arange(len(feature_inds), 0, -1)\n",
    "    feature_names_new = []\n",
    "    for inds in orig_inds:\n",
    "        if len(inds) == 1:\n",
    "            feature_names_new.append(feature_names[inds[0]])\n",
    "        else:\n",
    "            full_print = \" + \".join([feature_names[i] for i in inds])\n",
    "            if len(full_print) <= 40:\n",
    "                feature_names_new.append(full_print)\n",
    "            else:\n",
    "                max_ind = np.argmax(np.abs(orig_values).mean(0)[inds])\n",
    "                feature_names_new.append(f\"{feature_names[inds[max_ind]]} + {len(inds) - 1} other features\")\n",
    "    feature_names = feature_names_new\n",
    "\n",
    "    # see how many individual (vs. grouped at the end) features we are plotting\n",
    "    if num_features < len(values[0]):\n",
    "        num_cut = np.sum([len(orig_inds[feature_order[i]]) for i in range(num_features - 1, len(values[0]))])\n",
    "        values[:, feature_order[num_features - 1]] = np.sum(\n",
    "            [values[:, feature_order[i]] for i in range(num_features - 1, len(values[0]))], 0\n",
    "        )\n",
    "\n",
    "    # build our y-tick labels\n",
    "    yticklabels = []\n",
    "    for i in feature_inds:\n",
    "        if features is not None and show_data:\n",
    "            yticklabels.append(format_value(features[i], \"%0.03f\") + \" = \" + feature_names[i])\n",
    "        else:\n",
    "            yticklabels.append(feature_names[i])\n",
    "    if num_features < len(values[0]):\n",
    "        yticklabels[-1] = f\"Sum of {num_cut} other features\"\n",
    "\n",
    "    if ax is None:\n",
    "        ax = pl.gca()\n",
    "        # Only modify the figure size if ax was not passed in\n",
    "        # compute our figure size based on how many features we are showing\n",
    "        fig = pl.gcf()\n",
    "        row_height = 0.5\n",
    "        fig.set_size_inches(8, num_features * row_height * np.sqrt(len(values)) + 1.5)\n",
    "\n",
    "    # if negative values are present then we draw a vertical line to mark 0, otherwise the axis does this for us...\n",
    "    negative_values_present = np.sum(values[:, feature_order[:num_features]] < 0) > 0\n",
    "    if negative_values_present:\n",
    "        ax.axvline(0, 0, 1, color=\"#000000\", linestyle=\"-\", linewidth=1, zorder=1)\n",
    "\n",
    "    # draw the bars\n",
    "    patterns = (None, \"\\\\\\\\\", \"++\", \"xx\", \"////\", \"*\", \"o\", \"O\", \".\", \"-\")\n",
    "    total_width = 0.7\n",
    "    bar_width = total_width / len(values)\n",
    "    for i in range(len(values)):\n",
    "        ypos_offset = -((i - len(values) / 2) * bar_width + bar_width / 2)\n",
    "        ax.barh(\n",
    "            y_pos + ypos_offset,\n",
    "            values[i, feature_inds],\n",
    "            bar_width,\n",
    "            align=\"center\",\n",
    "            color=[\n",
    "                style.primary_color_negative if values[i, feature_inds[j]] <= 0 else postive_color\n",
    "                for j in range(len(y_pos))\n",
    "            ],\n",
    "            hatch=patterns[i],\n",
    "            edgecolor=(1, 1, 1, 0.8),\n",
    "            label=f\"{cohort_labels[i]} [{cohort_sizes[i] if i < len(cohort_sizes) else None}]\",\n",
    "        )\n",
    "\n",
    "    # draw the yticks (the 1e-8 is so matplotlib 3.3 doesn't try and collapse the ticks)\n",
    "    ax.set_yticks(list(y_pos) + list(y_pos + 1e-8), yticklabels + [t.split(\"=\")[-1] for t in yticklabels], fontsize=13)\n",
    "\n",
    "    xlen = ax.get_xlim()[1] - ax.get_xlim()[0]\n",
    "    # xticks = ax.get_xticks()\n",
    "    bbox = ax.get_window_extent().transformed(ax.figure.dpi_scale_trans.inverted())\n",
    "    width = bbox.width\n",
    "    bbox_to_xscale = xlen / width\n",
    "\n",
    "    for i in range(len(values)):\n",
    "        ypos_offset = -((i - len(values) / 2) * bar_width + bar_width / 2)\n",
    "        for j in range(len(y_pos)):\n",
    "            ind = feature_order[j]\n",
    "            if values[i, ind] < 0:\n",
    "                ax.text(\n",
    "                    values[i, ind] - (5 / 72) * bbox_to_xscale,\n",
    "                    y_pos[j] + ypos_offset,\n",
    "                    format_value(values[i, ind], \"%+0.02f\"),\n",
    "                    horizontalalignment=\"right\",\n",
    "                    verticalalignment=\"center\",\n",
    "                    color=style.primary_color_negative,\n",
    "                    fontsize=12,\n",
    "                )\n",
    "            else:\n",
    "                ax.text(\n",
    "                    values[i, ind] + (5 / 72) * bbox_to_xscale,\n",
    "                    y_pos[j] + ypos_offset,\n",
    "                    format_value(values[i, ind], \"%+0.02f\"),\n",
    "                    horizontalalignment=\"left\",\n",
    "                    verticalalignment=\"center\",\n",
    "                    color=postive_color,\n",
    "                    fontsize=12,\n",
    "                )\n",
    "\n",
    "    # put horizontal lines for each feature row\n",
    "    for i in range(num_features):\n",
    "        ax.axhline(i + 1, color=\"#888888\", lw=0.5, dashes=(1, 5), zorder=-1)\n",
    "\n",
    "    if features is not None:\n",
    "        features = list(features)\n",
    "\n",
    "        # try and round off any trailing zeros after the decimal point in the feature values\n",
    "        for i in range(len(features)):\n",
    "            try:\n",
    "                if round(features[i]) == features[i]:\n",
    "                    features[i] = int(features[i])\n",
    "            except Exception:\n",
    "                pass  # features[i] must not be a number\n",
    "\n",
    "    ax.xaxis.set_ticks_position(\"bottom\")\n",
    "    ax.yaxis.set_ticks_position(\"none\")\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    if negative_values_present:\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "    ax.tick_params(\"x\", labelsize=11)\n",
    "\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    x_buffer = (xmax - xmin) * 0.05\n",
    "\n",
    "    if negative_values_present:\n",
    "        ax.set_xlim(xmin - x_buffer, xmax + x_buffer)\n",
    "    else:\n",
    "        ax.set_xlim(xmin, xmax + x_buffer)\n",
    "\n",
    "    # if features is None:\n",
    "    #     pl.xlabel(labels[\"GLOBAL_VALUE\"], fontsize=13)\n",
    "    # else:\n",
    "    ax.set_xlabel(xlabel, fontsize=13)\n",
    "\n",
    "    if len(values) > 1:\n",
    "        ax.legend(fontsize=12)\n",
    "\n",
    "    # color the y tick labels that have the feature values as gray\n",
    "    # (these fall behind the black ones with just the feature name)\n",
    "    tick_labels = ax.yaxis.get_majorticklabels()\n",
    "    for i in range(num_features):\n",
    "        tick_labels[i].set_color(style.tick_labels_color)\n",
    "\n",
    "    # draw a dendrogram if we are given a partition tree\n",
    "    if partition_tree is not None:\n",
    "        # compute the dendrogram line positions based on our current feature order\n",
    "        feature_pos = np.argsort(feature_order)\n",
    "        ylines, xlines = dendrogram_coords(feature_pos, partition_tree)\n",
    "\n",
    "        # plot the distance cut line above which we don't show tree edges\n",
    "        xmin, xmax = ax.get_xlim()\n",
    "        xlines_min, xlines_max = np.min(xlines), np.max(xlines)\n",
    "        ct_line_pos = (clustering_cutoff / (xlines_max - xlines_min)) * 0.1 * (xmax - xmin) + xmax\n",
    "        ax.text(\n",
    "            ct_line_pos + 0.005 * (xmax - xmin),\n",
    "            (ymax - ymin) / 2,\n",
    "            \"Clustering cutoff = \" + format_value(clustering_cutoff, \"%0.02f\"),\n",
    "            horizontalalignment=\"left\",\n",
    "            verticalalignment=\"center\",\n",
    "            color=\"#999999\",\n",
    "            fontsize=12,\n",
    "            rotation=-90,\n",
    "        )\n",
    "        line = ax.axvline(ct_line_pos, color=\"#dddddd\", dashes=(1, 1))\n",
    "        line.set_clip_on(False)\n",
    "\n",
    "        for xline, yline in zip(xlines, ylines):\n",
    "            # normalize the x values to fall between 0 and 1\n",
    "            xv = np.array(xline) / (xlines_max - xlines_min)\n",
    "\n",
    "            # only draw if we are not going past distance threshold\n",
    "            if np.array(xline).max() <= clustering_cutoff:\n",
    "                # only draw if we are not going past the bottom of the plot\n",
    "                if yline.max() < max_display:\n",
    "                    lines = ax.plot(xv * 0.1 * (xmax - xmin) + xmax, max_display - np.array(yline), color=\"#999999\")\n",
    "                    for line in lines:\n",
    "                        line.set_clip_on(False)\n",
    "\n",
    "    if show:\n",
    "        pl.show()\n",
    "    else:\n",
    "        return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Global feature importance plot\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "bar(shap_values, max_display=20, ax=ax)\n",
    "fig.savefig(PROJECT_PATH / 'figures/azt_regression/shap_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Detailed feature importance visualization\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "beeswarm(shap_values, max_display=20)\n",
    "fig.savefig(PROJECT_PATH / 'figures/azt_regression/shap_beeswarm.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "# Randomly sample 1000 instances from X_test\n",
    "random_indices = np.random.choice(len(X_test), size=1000, replace=False)\n",
    "shap_values = explainer(X_test.iloc[random_indices])\n",
    "\n",
    "# Feature interaction analysis\n",
    "ax = shap.plots.heatmap(shap_values, max_display=20, show=False)\n",
    "\n",
    "# Modify the colorbar\n",
    "cb = ax.figure.axes[-1]  # Get the colorbar axes (last axes in the figure)\n",
    "cb.set_box_aspect(30)  # Make colorbar thicker\n",
    "\n",
    "# Increase padding between colorbar and main axes\n",
    "cb.set_position([cb.get_position().x0, 0.29, cb.get_position().width+10, 0.35])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "ax.figure.savefig(PROJECT_PATH / 'figures/azt_regression/shap_interaction_heatmap.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "# Get the top 2000 instances based on y_test values\n",
    "top_indices = np.argsort(y_pred)[-553:]  # Get indices of highest y_test values\n",
    "shap_values = explainer(X_test.iloc[top_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is equivalent to how SHAP calculates the order internally\n",
    "from shap.utils import hclust_ordering\n",
    "\n",
    "# Get the ordering of instances\n",
    "instance_order = hclust_ordering(shap_values.values)\n",
    "\n",
    "# Get first 10 and their features\n",
    "first_10_examples = X_test.iloc[top_indices[instance_order][:10]]\n",
    "print(\"\\nTop 10 examples with highest predicted fitness:\")\n",
    "print(first_10_examples)\n",
    "\n",
    "print(df_genotypes[first_10_examples.index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Feature interaction analysis\n",
    "ax = shap.plots.heatmap(shap_values, max_display=20, show=False)\n",
    "\n",
    "# Modify the colorbar\n",
    "cb = ax.figure.axes[-1]  # Get the colorbar axes (last axes in the figure)\n",
    "cb.set_box_aspect(30)    # Make colorbar thicker\n",
    "cb.set_position([cb.get_position().x0, 0.29, cb.get_position().width+0.01, 0.35])\n",
    "\n",
    "# --- add a cutout inset zooming into the first 30 instances ---\n",
    "# grab the heatmap image and its clustered data array\n",
    "im = ax.images[0]\n",
    "heatmap_data = im.get_array()\n",
    "\n",
    "# create inset axes in the upper‐right corner of the main axes\n",
    "# Fix: provide a 4-tuple (x, y, width, height) for bbox_to_anchor when using relative size\n",
    "axins = inset_axes(ax, width=\"50%\", height=\"100%\", loc='upper right',\n",
    "                   bbox_to_anchor=(-0.69, .2, 0.55, .6), bbox_transform=ax.transAxes)\n",
    "# display the same heatmap data but only first 9 columns\n",
    "axins.imshow(heatmap_data, aspect='auto', cmap=im.cmap, norm=im.norm)\n",
    "axins.set_xlim(-.5, 9.5)\n",
    "# invert y‐axis so rows align with main plot\n",
    "axins.set_ylim(heatmap_data.shape[0]-0.5, -0.5)\n",
    "\n",
    "# Add tick labels for better readability\n",
    "axins.set_xticks(np.arange(0,10))\n",
    "axins.set_xticklabels(df_genotypes[first_10_examples.index], rotation=90, fontsize=9, fontfamily='monospace')\n",
    "\n",
    "# Set y-ticks for axins\n",
    "y_positions = range(heatmap_data.shape[0])\n",
    "# Use a subset of y positions to avoid overcrowding\n",
    "axins.set_yticks(range(len(feature_importance)))\n",
    "axins.set_yticklabels([feat for feat in feature_importance['feature']], fontsize=9)\n",
    "axins.tick_params(axis='both', length=3, width=0.5)\n",
    "\n",
    "# Enhance appearance\n",
    "axins.spines['top'].set_visible(True)\n",
    "axins.spines['right'].set_visible(True)\n",
    "axins.spines['left'].set_visible(True)\n",
    "axins.spines['bottom'].set_visible(True)\n",
    "\n",
    "# # Add a rectangle to the main axes to indicate the zoomed (cutout) region\n",
    "# rect = Rectangle((0.49, -.49), 9, heatmap_data.shape[0]-.1, linewidth=0.75, edgecolor='black', facecolor='none', linestyle='--')\n",
    "# ax.add_patch(rect)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Uncomment to save the figure\n",
    "ax.figure.savefig(PROJECT_PATH / 'figures/azt_regression/shap_interaction_heatmap.png',\n",
    "                  dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Use a reasonable sample size for analysis\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Convert SHAP values to a DataFrame for correlation analysis\n",
    "shap_df = pd.DataFrame(shap_values.values, columns=X_test.columns)\n",
    "\n",
    "# Calculate correlation matrix between SHAP values\n",
    "shap_corr = shap_df.corr()\n",
    "\n",
    "# Drop features with NaN correlations\n",
    "shap_corr = shap_corr.dropna(how='all').dropna(axis=1, how='all')\n",
    "\n",
    "# Create a correlation heatmap\n",
    "plt.figure(figsize=(12, 10), dpi=90)\n",
    "mask = np.triu(np.ones_like(shap_corr, dtype=bool))  # Create mask for upper triangle\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)  # Create colormap\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(shap_corr, mask=mask, cmap=shap.plots.colors.red_white_blue, vmax=.5, vmin=-.5, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=False)\n",
    "\n",
    "plt.title('SHAP Value Correlations Between Feature Pairs', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_PATH / 'figures/azt_regression/shap_correlation_heatmap.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgbm312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
